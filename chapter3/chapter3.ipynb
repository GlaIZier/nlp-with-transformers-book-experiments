{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67df23c1-848b-4953-9a70-8711eb0ace6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2051, 10029,  2066,  2019,  8612]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "model_ckpt = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "text = \"time flies like an arrow\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)\n",
    "inputs.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08fad7a1-0022-49db-9140-aef6e07276c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(30522, 768)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "from transformers import AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_ckpt)\n",
    "token_emb = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "token_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0db19ff3-5a74-4c5f-aaf5-a78be9aeb24f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 768])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_embeds = token_emb(inputs.input_ids)\n",
    "inputs_embeds.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b0bd46f-5503-4ec8-a8bd-3187b4e6abe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 5])\n",
      "tensor([[1., 1., 1., 1., 1.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from math import sqrt\n",
    "query = key = value = inputs_embeds\n",
    "dim_k = key.size(-1)\n",
    "scores = torch.bmm(query, key.transpose(1,2)) / sqrt(dim_k)\n",
    "print(scores.size())\n",
    "\n",
    "import torch.nn.functional as F \n",
    "weights = F.softmax(scores, dim=-1)\n",
    "print(weights.sum(dim=-1))\n",
    "\n",
    "attn_outputs = torch.bmm(weights, value)\n",
    "print(attn_outputs.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d2b4b1-ca6e-458a-9ccb-03f8c79a56c1",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6c7c476c-aafe-4835-a5ca-d01a0533e247",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention_without_mask(query, key, value):\n",
    "    dim_k = query.size(-1)\n",
    "    scores = torch.bmm(query, key.transpose(1, 2)) / sqrt(dim_k) \n",
    "    weights = F.softmax(scores, dim=-1)\n",
    "    return torch.bmm(weights, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "440eb9a8-b0aa-48e2-ba5b-6460fe3c1f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# self-attention layer applies three independent linear transformations to each embedding to generate the query, key, and value vectors. These transformations project the embeddings and each projection carries its own set of learnable parameters, which allows the self-attention layer to focus on different semantic aspects of the sequence.\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, embed_dim, head_dim):\n",
    "        # [batch_size, seq_len, head_dim]\n",
    "        super().__init__()\n",
    "        self.q = nn.Linear(embed_dim, head_dim)\n",
    "        self.k = nn.Linear(embed_dim, head_dim)\n",
    "        self.v = nn.Linear(embed_dim, head_dim)\n",
    "    def forward(self, hidden_state):\n",
    "        attn_outputs = scaled_dot_product_attention(self.q(hidden_state), self.k(hidden_state), self.v(hidden_state))\n",
    "        return attn_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0163261a-61d2-4fdd-bf3b-67c63081e99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# beneficial to have multiple sets of linear projections, each one representing a so-called attention head.\n",
    "class MultiHeadAttention(nn.Module): \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        embed_dim = config.hidden_size\n",
    "        num_heads = config.num_attention_heads\n",
    "        head_dim = embed_dim // num_heads\n",
    "        self.heads = nn.ModuleList([AttentionHead(embed_dim, head_dim) for _ in range(num_heads)])\n",
    "        self.output_linear = nn.Linear(embed_dim, embed_dim)\n",
    "    def forward(self, hidden_state):\n",
    "        x = torch.cat([h(hidden_state) for h in self.heads], dim=-1)\n",
    "        x = self.output_linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc08ddda-4496-4e63-a9e1-ff44ccee41c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 768])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multihead_attn = MultiHeadAttention(config)\n",
    "attn_output = multihead_attn(inputs_embeds)\n",
    "attn_output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5f52651a-0d3c-4fa4-8d17-1a8d6d9f4d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  instead of processing the whole sequence of embeddings as a single vector, it processes each embedding independently. For this reason, this layer is often referred to as a position-wise feed-forward layer. You may also see it referred to as a one-dimensional convolution with a kernel size of one. A rule of thumb from the literature is for the hidden size of the first layer to be four times the size of the embeddings, and a GELU activation function is most commonly used. This is where most of the capacity and memorization is hypothesized to happen, and itâ€™s the part that is most often scaled when scaling up the models.\n",
    "\n",
    "class FeedForward(nn.Module): \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # 'DistilBertConfig' object has no attribute 'intermediate_size'\n",
    "        self.linear_1 = nn.Linear(config.hidden_size, 3072)\n",
    "        self.linear_2 = nn.Linear(3072, config.hidden_size)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "    def forward(self, x):\n",
    "        x = self.linear_1(x) \n",
    "        x = self.gelu(x)\n",
    "        x = self.linear_2(x) \n",
    "        x = self.dropout(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fea2e4cd-8d18-4951-9ef2-5e2e49643473",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 768])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nn.Linear is usually applied to a tensor of shape (batch_size, input_dim), where it acts on each element of the batch dimension independently. This is actually true for any dimension except the last one, so when we pass a tensor of shape (batch_size, seq_len, hidden_dim) the layer is applied to all token embeddings of the batch and sequence independently\n",
    "feed_forward = FeedForward(config)\n",
    "ff_outputs = feed_forward(attn_outputs)\n",
    "ff_outputs.size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a53709d6-95da-4a11-863a-12e9e2acf4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module): \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.layer_norm_1 = nn.LayerNorm(config.hidden_size)\n",
    "        self.layer_norm_2 = nn.LayerNorm(config.hidden_size)\n",
    "        self.attention = MultiHeadAttention(config)\n",
    "        self.feed_forward = FeedForward(config)\n",
    "    def forward(self, x):\n",
    "        # Apply layer normalization and then copy input into query, key, value\n",
    "        hidden_state = self.layer_norm_1(x)\n",
    "        # Apply attention with a skip connection\n",
    "        x = x + self.attention(hidden_state)\n",
    "        # Apply feed-forward layer with a skip connection \n",
    "        x = x + self.feed_forward(self.layer_norm_2(x)) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2ea9b757-fae1-49b3-b401-b2ee8c1d426f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 5, 768]), torch.Size([1, 5, 768]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_layer = TransformerEncoderLayer(config)\n",
    "inputs_embeds.shape, encoder_layer(inputs_embeds).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a0c7f8e9-0b25-4535-ba63-84ca363b4d56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 768])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Embeddings(nn.Module): \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.token_embeddings = nn.Embedding(config.vocab_size,  config.hidden_size)\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
    "        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout()\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        # Create position IDs for input sequence \n",
    "        seq_length = input_ids.size(1) \n",
    "        position_ids = torch.arange(seq_length, dtype=torch.long).unsqueeze(0)\n",
    "        # Create token and position embeddings \n",
    "        token_embeddings = self.token_embeddings(input_ids) \n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        # Combine token and position embeddings\n",
    "        embeddings = token_embeddings + position_embeddings \n",
    "        embeddings = self.layer_norm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings\n",
    "\n",
    "embedding_layer = Embeddings(config)\n",
    "embedding_layer(inputs.input_ids).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1f63cfea-c4c2-4819-aa1e-982143ff4cb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 768])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TransformerEncoder(nn.Module): \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embeddings = Embeddings(config)\n",
    "        self.layers = nn.ModuleList([TransformerEncoderLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x) \n",
    "        for layer in self.layers:\n",
    "            x = layer(x) \n",
    "        return x\n",
    "\n",
    "encoder = TransformerEncoder(config)\n",
    "encoder(inputs.input_ids).size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a0f38755-9ed1-4ab7-b140-eb69c82b1db6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TransformerForSequenceClassification(nn.Module): \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.encoder = TransformerEncoder(config)\n",
    "        self.dropout = nn.Dropout(0.1) # DistilBert doesn't have config.hidden_dropout_prob\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)[:, 0, :] # select hidden state of [CLS] token\n",
    "        x = self.dropout(x)\n",
    "        x = self.classifier(x) \n",
    "        return x\n",
    "\n",
    "config.num_labels = 3\n",
    "encoder_classifier = TransformerForSequenceClassification(config)\n",
    "encoder_classifier(inputs.input_ids).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927136bb-7dcf-4c5f-a781-a47a4cae8cbb",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5aebb63d-c9b4-45d9-b128-39e3a7e8dd34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_len = inputs.input_ids.size(-1)\n",
    "mask = torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0)\n",
    "mask[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b0e00bc1-201b-4941-a673-5535de0b08b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[27.2641,    -inf,    -inf,    -inf,    -inf],\n",
       "         [ 0.5911, 27.7673,    -inf,    -inf,    -inf],\n",
       "         [ 0.7643, -0.1611, 27.4965,    -inf,    -inf],\n",
       "         [ 0.1593, -0.0332,  0.3982, 27.4464,    -inf],\n",
       "         [ 0.0527, -0.1541, -0.2480, -0.3516, 30.6703]]],\n",
       "       grad_fn=<MaskedFillBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.masked_fill(mask == 0, -float(\"inf\")) # attention weights are all zero once we take the softmax over the scores because "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cbbe6c5e-00f3-4067-aecf-9906b9e94805",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value, mask=None): \n",
    "    dim_k = query.size(-1)\n",
    "    scores = torch.bmm(query, key.transpose(1, 2)) / sqrt(dim_k) \n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, float(\"-inf\")) \n",
    "    weights = F.softmax(scores, dim=-1)\n",
    "    return weights.bmm(value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
